# goit-de-hw-04. Apache Spark. –û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è —Ç–∞ SparkU–Ü

## –ó–∞–≤–¥–∞–Ω–Ω—è

- –∑–∞–ø—É—Å—Ç–∏—Ç–∏ —Ç—Ä–∏ –ø—Ä–æ–≥—Ä–∞–º–∏;
- –∑—Ä–æ–±–∏—Ç–∏ —Å–∫—Ä–∏–Ω—à–æ—Ç–∏ —Ç—Ä—å–æ—Ö –Ω–∞–±–æ—Ä—ñ–≤ Jobs;
- –ø—Ä–æ–∞–Ω–∞–ª—ñ–∑—É–≤–∞—Ç–∏ —Ç–∞ –≤–º—ñ—Ç–∏ –æ–±“ë—Ä—É–Ω—Ç—É–≤–∞—Ç–∏ –Ω–∞—è–≤–Ω—ñ—Å—Ç—å –ø–µ–≤–Ω–æ—ó –∫—ñ–ª—å–∫–æ—Å—Ç—ñ Jobs —É –∫–æ–∂–Ω–æ–º—É –Ω–∞–±–æ—Ä—ñ;
- –∑—Ä–æ–∑—É–º—ñ—Ç–∏, —â–æ —Ä–æ–±–∏—Ç—å —Ñ—É–Ω–∫—Ü—ñ—è `cache` —Ç–∞ –Ω–∞–≤—ñ—â–æ —ó—ó –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏.

## –†—ñ—à–µ–Ω–Ω—è

### –ß–∞—Å—Ç–∏–Ω–∞ 1

–ó–∞–ø—É—Å—Ç—ñ—Ç—å –∫–æ–¥. –ó—Ä–æ–±—ñ—Ç—å —Å–∫—Ä–∏–Ω—à–æ—Ç —É—Å—ñ—Ö Jobs (—ó—Ö –º–∞—î –±—É—Ç–∏ 5).

```Py
from pyspark.sql import SparkSession

# –°—Ç–≤–æ—Ä—é—î–º–æ —Å–µ—Å—ñ—é Spark
spark = (
    SparkSession.builder.master("local[*]")
    .config("spark.sql.shuffle.partitions", "2")
    .appName("MyGoitSparkSandbox")
    .getOrCreate()
)

# –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –¥–∞—Ç–∞—Å–µ—Ç
nuek_df = (
    spark.read.option("header", "true")
    .option("inferSchema", "true")
    .csv("./csv/nuek-vuh3.csv")
)

nuek_repart = nuek_df.repartition(2)

nuek_processed = (
    nuek_repart.where("final_priority < 3")
    .select("unit_id", "final_priority")
    .groupBy("unit_id")
    .count()
)

# –û—Å—å –¢–£–¢ –¥–æ–¥–∞–Ω–æ —Ä—è–¥–æ–∫
nuek_processed = nuek_processed.where("count>2")

nuek_processed.collect()

input("Press Enter to continue...5")

# –ó–∞–∫—Ä–∏–≤–∞—î–º–æ —Å–µ—Å—ñ—é Spark
spark.stop()
```

–°–∫—Ä—ñ–Ω—à–æ—Ç–∏ Jobs:

1. [Job 0](md.media/p1_j0.png)
2. [Job 1](md.media/p1_j1.png)
3. [Job 2](md.media/p1_j2.png)
4. [Job 3](md.media/p1_j3.png)
5. [Job 4](md.media/p1_j4.png)

### –ß–∞—Å—Ç–∏–Ω–∞ 2

–î–æ–¥–∞–º–æ –ø—Ä–æ–º—ñ–∂–Ω–∏–π Action ‚Äî collect:

```Py
from pyspark.sql import SparkSession

# –°—Ç–≤–æ—Ä—é—î–º–æ —Å–µ—Å—ñ—é Spark
spark = (
    SparkSession.builder.master("local[*]")
    .config("spark.sql.shuffle.partitions", "2")
    .appName("MyGoitSparkSandbox")
    .getOrCreate()
)

# –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –¥–∞—Ç–∞—Å–µ—Ç
nuek_df = (
    spark.read.option("header", "true")
    .option("inferSchema", "true")
    .csv("./csv/nuek-vuh3.csv")
)

nuek_repart = nuek_df.repartition(2)

nuek_processed = (
    nuek_repart.where("final_priority < 3")
    .select("unit_id", "final_priority")
    .groupBy("unit_id")
    .count()
)

# –ü—Ä–æ–º—ñ–∂–Ω–∏–π action: collect
nuek_processed.collect()

# –û—Å—å –¢–£–¢ –¥–æ–¥–∞–Ω–æ —Ä—è–¥–æ–∫
nuek_processed = nuek_processed.where("count>2")

nuek_processed.collect()

input("Press Enter to continue...5")

# –ó–∞–∫—Ä–∏–≤–∞—î–º–æ —Å–µ—Å—ñ—é Spark
spark.stop()
```

`Collect` —î –æ–¥–Ω—ñ—î—é –∑ –¥—ñ–π (Action) —è–∫—ñ –∑–∞–ø—É—Å–∫–∞—é—Ç—å Job.  
–¢–∞–∫–∏–º–∏ –¥—ñ—è–º–∏ –≤ Spark —Ç–∞–∫–æ–∂ —î: `collect()`, `count()`, `save()`, `show()`

–°–∫—Ä—ñ–Ω—à–æ—Ç–∏ Jobs:

1. [Job 0](md.media/p2_0.png)
2. [Job 1](md.media/p2_1.png)
3. [Job 2](md.media/p2_2.png)
4. [Job 3](md.media/p2_3.png)
5. [Job 4](md.media/p2_4.png)
6. [Job 5](md.media/p2_5.png)
7. [Job 6](md.media/p2_6.png)
8. [Job 7](md.media/p2_7.png)

### –ß–∞—Å—Ç–∏–Ω–∞ 3

–í–∏–∫–æ—Ä–∏—Å—Ç–∞—î–º–æ —Ñ—É–Ω–∫—Ü—ñ—é `cache` –≤ –ø—Ä–æ–º—ñ–∂–Ω–æ–º—É —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ.

     ‚òùüèª–§—É–Ω–∫—Ü—ñ—è cache() –≤ PySpark –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –¥–ª—è –∫–µ—à—É–≤–∞–Ω–Ω—è (–∑–±–µ—Ä—ñ–≥–∞–Ω–Ω—è –≤ –ø–∞–º'—è—Ç—ñ) –¥–∞–Ω–∏—Ö RDD (Resilient Distributed Dataset) –∞–±–æ DataFrame. –¶–µ –¥–æ–∑–≤–æ–ª—è—î –ø—Ä–∏—Å–∫–æ—Ä–∏—Ç–∏ –≤–∏–∫–æ–Ω–∞–Ω–Ω—è –ø–æ–¥–∞–ª—å—à–∏—Ö –¥—ñ–π (actions) –∞–±–æ –ø–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω—å (transformations), —è–∫—ñ –ø—Ä–∞—Ü—é—é—Ç—å –∑ —Ç–∏–º–∏ –∂ –¥–∞–Ω–∏–º–∏. –ö–µ—à—É–≤–∞–Ω–Ω—è –æ—Å–æ–±–ª–∏–≤–æ –∫–æ—Ä–∏—Å–Ω–µ, –∫–æ–ª–∏ –≤–∏ –≤–∏–∫–æ–Ω—É—î—Ç–µ –¥–µ–∫—ñ–ª—å–∫–∞ –æ–ø–µ—Ä–∞—Ü—ñ–π –Ω–∞ –æ–¥–Ω–æ–º—É –π —Ç–æ–º—É –∂ RDD –∞–±–æ DataFrame, –æ—Å–∫—ñ–ª—å–∫–∏ PySpark –Ω–µ –±—É–¥–µ –ø–æ–≤—Ç–æ—Ä–Ω–æ –æ–±—á–∏—Å–ª—é–≤–∞—Ç–∏ —Ç—ñ —Å–∞–º—ñ –¥–∞–Ω—ñ.

**–Ø–∫ –ø—Ä–∞—Ü—é—î **`cache()`** :**

**1. –ö–µ—à—É–≤–∞–Ω–Ω—è –≤ –ø–∞–º'—è—Ç—ñ.** –ö–æ–ª–∏ –≤–∏ –≤–∏–∫–ª–∏–∫–∞—î—Ç–µ `cache()` –Ω–∞ RDD –∞–±–æ DataFrame, –¥–∞–Ω—ñ –∑–±–µ—Ä—ñ–≥–∞—é—Ç—å—Å—è –≤ –ø–∞–º'—è—Ç—ñ (RAM) —É —Ä–æ–∑–ø–æ–¥—ñ–ª–µ–Ω–æ–º—É –≤–∏–≥–ª—è–¥—ñ –Ω–∞ –≤—Å—ñ—Ö –≤—É–∑–ª–∞—Ö –∫–ª–∞—Å—Ç–µ—Ä–∞. –¶–µ –¥–æ–∑–≤–æ–ª—è—î –ø—Ä–∏—Å–∫–æ—Ä–∏—Ç–∏ –ø–æ–¥–∞–ª—å—à—ñ –æ–±—á–∏—Å–ª–µ–Ω–Ω—è, –æ—Å–∫—ñ–ª—å–∫–∏ Spark –Ω–µ –±—É–¥–µ –∑–Ω–æ–≤—É –∑–∞–≤–∞–Ω—Ç–∞–∂—É–≤–∞—Ç–∏ –∞–±–æ –æ–±—á–∏—Å–ª—é–≤–∞—Ç–∏ —Ü—ñ –¥–∞–Ω—ñ.

**2. –õ—ñ–Ω–∏–≤–µ –≤–∏–∫–æ–Ω–∞–Ω–Ω—è.** –í–∏–∫–ª–∏–∫ `cache()` –Ω–µ –ø—Ä–∏–∑–≤–æ–¥–∏—Ç—å –¥–æ –Ω–µ–≥–∞–π–Ω–æ–≥–æ –≤–∏–∫–æ–Ω–∞–Ω–Ω—è –æ–±—á–∏—Å–ª–µ–Ω—å. –õ–∏—à–µ –∫–æ–ª–∏ –≤–∏ –≤–∏–∫–æ–Ω—É—î—Ç–µ –¥—ñ—é (action), –Ω–∞–ø—Ä–∏–∫–ª–∞–¥, `count()`, `collect()`, –∞–±–æ `show()`, –¥–∞–Ω—ñ –±—É–¥—É—Ç—å –æ–±—á–∏—Å–ª–µ–Ω—ñ —Ç–∞ –∫–µ—à–æ–≤–∞–Ω—ñ.

**3. –ú–µ—Ö–∞–Ω—ñ–∑–º –∑–±–µ—Ä—ñ–≥–∞–Ω–Ω—è.** –ó–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º, `cache()` –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î –ø–∞–º'—è—Ç—å (Memory). –û–¥–Ω–∞–∫, —è–∫—â–æ –¥–∞–Ω—ñ –Ω–µ –ø–æ–º—ñ—â–∞—é—Ç—å—Å—è –≤ –ø–∞–º'—è—Ç—å, Spark –±—É–¥–µ –∑–±–µ—Ä—ñ–≥–∞—Ç–∏ —ó—Ö –Ω–∞ –¥–∏—Å–∫—É.

**4. –ö–æ–Ω—Ç—Ä–æ–ª—å –Ω–∞–¥ –∫–µ—à—É–≤–∞–Ω–Ω—è–º.** –ö–æ–ª–∏ –≤–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç–µ `cache()`, Spark –∑–±–µ—Ä—ñ–≥–∞—î –¥–∞–Ω—ñ –∑ —Ä—ñ–≤–Ω–µ–º –∑–±–µ—Ä—ñ–≥–∞–Ω–Ω—è `MEMORY_ONLY`. –Ø–∫—â–æ –≤–∏ —Ö–æ—á–µ—Ç–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ —ñ–Ω—à—ñ —Ä—ñ–≤–Ω—ñ –∑–±–µ—Ä—ñ–≥–∞–Ω–Ω—è, —Ç–∞–∫—ñ —è–∫ `MEMORY_AND_DISK`, –º–æ–∂–Ω–∞ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ –º–µ—Ç–æ–¥ `persist()`.

    ‚òùüèª–í–∞–∂–ª–∏–≤–æ –∑–Ω–∞—Ç–∏, —â–æ –º–æ–∂–ª–∏–≤–æ –∑–±–µ—Ä—ñ–≥–∞—Ç–∏ —è–∫ –≤ –ø–∞–º'—è—Ç—ñ, —Ç–∞–∫ —ñ –Ω–∞ –¥–∏—Å–∫—É. –ó–±–µ—Ä—ñ–≥–∞–Ω–Ω—è –≤ –ø–∞–º'—è—Ç—ñ –Ω–∞–±–∞–≥–∞—Ç–æ –±—ñ–ª—å—à —Ä–æ–∑–ø–æ–≤—Å—é–¥–∂–µ–Ω–µ, –Ω–∞ –¥–∏—Å–∫—É ‚Äî –µ–∫–∑–æ—Ç–∏–∫–∞ üòâ

```Py
from pyspark.sql import SparkSession

# –°—Ç–≤–æ—Ä—é—î–º–æ —Å–µ—Å—ñ—é Spark
spark = (
    SparkSession.builder.master("local[*]")
    .config("spark.sql.shuffle.partitions", "2")
    .appName("MyGoitSparkSandbox")
    .getOrCreate()
)

# –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –¥–∞—Ç–∞—Å–µ—Ç
nuek_df = (
    spark.read.option("header", "true")
    .option("inferSchema", "true")
    .csv("./csv/nuek-vuh3.csv")
)

nuek_repart = nuek_df.repartition(2)

nuek_processed_cached = (
    nuek_repart.where("final_priority < 3")
    .select("unit_id", "final_priority")
    .groupBy("unit_id")
    .count()
    .cache()
)  # –î–æ–¥–∞–Ω–æ —Ñ—É–Ω–∫—Ü—ñ—é cache

# –ü—Ä–æ–º—ñ–∂–Ω–∏–π action: collect
nuek_processed_cached.collect()

# –û—Å—å –¢–£–¢ –¥–æ–¥–∞–Ω–æ —Ä—è–¥–æ–∫
nuek_processed = nuek_processed_cached.where("count>2")

nuek_processed.collect()

input("Press Enter to continue...5")


# –ó–≤—ñ–ª—å–Ω—è—î–º–æ –ø–∞–º'—è—Ç—å –≤—ñ–¥ Dataframe
nuek_processed_cached.unpersist()

# –ó–∞–∫—Ä–∏–≤–∞—î–º–æ —Å–µ—Å—ñ—é Spark
spark.stop()
```

–ó–∞–≤–¥—è–∫–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—é `cache` –º–∏ –∑–º–µ–Ω—à–∏–ª–∏ –∫—ñ–ª—å–∫—ñ—Å—Ç—å Jobs.

–°–∫—Ä—ñ–Ω—à–æ—Ç–∏ Jobs:

1. [All jobs](md.media/p3_jobs.png)
1. [Job 0](md.media/p3_job_0.png)
1. [Job 1](md.media/p3_job_1.png)
1. [Job 2](md.media/p3_job_2.png)
1. [Job 3](md.media/p3_job_3.png)

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# goit-de-hw-04. Apache Spark. –û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è —Ç–∞ SparkU–Ü\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ó–∞–≤–¥–∞–Ω–Ω—è\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- –∑–∞–ø—É—Å—Ç–∏—Ç–∏ —Ç—Ä–∏ –ø—Ä–æ–≥—Ä–∞–º–∏;\n",
    "- –∑—Ä–æ–±–∏—Ç–∏ —Å–∫—Ä–∏–Ω—à–æ—Ç–∏ —Ç—Ä—å–æ—Ö –Ω–∞–±–æ—Ä—ñ–≤ Jobs;\n",
    "- –ø—Ä–æ–∞–Ω–∞–ª—ñ–∑—É–≤–∞—Ç–∏ —Ç–∞ –≤–º—ñ—Ç–∏ –æ–±“ë—Ä—É–Ω—Ç—É–≤–∞—Ç–∏ –Ω–∞—è–≤–Ω—ñ—Å—Ç—å –ø–µ–≤–Ω–æ—ó –∫—ñ–ª—å–∫–æ—Å—Ç—ñ Jobs —É –∫–æ–∂–Ω–æ–º—É –Ω–∞–±–æ—Ä—ñ;\n",
    "- –∑—Ä–æ–∑—É–º—ñ—Ç–∏, —â–æ —Ä–æ–±–∏—Ç—å —Ñ—É–Ω–∫—Ü—ñ—è `cache` —Ç–∞ –Ω–∞–≤—ñ—â–æ —ó—ó –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –†—ñ—à–µ–Ω–Ω—è\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ß–∞—Å—Ç–∏–Ω–∞ 1\n",
    "\n",
    "–ó–∞–ø—É—Å—Ç—ñ—Ç—å –∫–æ–¥. –ó—Ä–æ–±—ñ—Ç—å —Å–∫—Ä–∏–Ω—à–æ—Ç —É—Å—ñ—Ö Jobs (—ó—Ö –º–∞—î –±—É—Ç–∏ 5).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(unit_id='83', count=4)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# –°—Ç–≤–æ—Ä—é—î–º–æ —Å–µ—Å—ñ—é Spark\n",
    "spark = (\n",
    "    SparkSession.builder.master(\"local[*]\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"2\")\n",
    "    .appName(\"MyGoitSparkSandbox\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –¥–∞—Ç–∞—Å–µ—Ç\n",
    "nuek_df = (\n",
    "    spark.read.option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .csv(\"./csv/nuek-vuh3.csv\")\n",
    ")\n",
    "\n",
    "nuek_repart = nuek_df.repartition(2)\n",
    "\n",
    "nuek_processed = (\n",
    "    nuek_repart.where(\"final_priority < 3\")\n",
    "    .select(\"unit_id\", \"final_priority\")\n",
    "    .groupBy(\"unit_id\")\n",
    "    .count()\n",
    ")\n",
    "\n",
    "# –û—Å—å –¢–£–¢ –¥–æ–¥–∞–Ω–æ —Ä—è–¥–æ–∫\n",
    "nuek_processed = nuek_processed.where(\"count>2\")\n",
    "\n",
    "nuek_processed.collect()\n",
    "\n",
    "# input(\"Press Enter to continue...5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–°–∫—Ä—ñ–Ω—à–æ—Ç–∏ Jobs:  \n",
    "\n",
    "1. [Job 0](md.media/p1_j0.png)  \n",
    "2. [Job 1](md.media/p1_j1.png)  \n",
    "3. [Job 2](md.media/p1_j2.png)  \n",
    "4. [Job 3](md.media/p1_j3.png)  \n",
    "5. [Job 4](md.media/p1_j4.png)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ó–∞–∫—Ä–∏–≤–∞—î–º–æ —Å–µ—Å—ñ—é Spark\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ß–∞—Å—Ç–∏–Ω–∞ 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–î–æ–¥–∞–º–æ –ø—Ä–æ–º—ñ–∂–Ω–∏–π Action ‚Äî collect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(unit_id='83', count=4)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# –°—Ç–≤–æ—Ä—é—î–º–æ —Å–µ—Å—ñ—é Spark\n",
    "spark = (\n",
    "    SparkSession.builder.master(\"local[*]\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"2\")\n",
    "    .appName(\"MyGoitSparkSandbox\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –¥–∞—Ç–∞—Å–µ—Ç\n",
    "nuek_df = (\n",
    "    spark.read.option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .csv(\"./csv/nuek-vuh3.csv\")\n",
    ")\n",
    "\n",
    "nuek_repart = nuek_df.repartition(2)\n",
    "\n",
    "nuek_processed = (\n",
    "    nuek_repart.where(\"final_priority < 3\")\n",
    "    .select(\"unit_id\", \"final_priority\")\n",
    "    .groupBy(\"unit_id\")\n",
    "    .count()\n",
    ")\n",
    "\n",
    "# –ü—Ä–æ–º—ñ–∂–Ω–∏–π action: collect\n",
    "nuek_processed.collect()\n",
    "\n",
    "# –û—Å—å –¢–£–¢ –¥–æ–¥–∞–Ω–æ —Ä—è–¥–æ–∫\n",
    "nuek_processed = nuek_processed.where(\"count>2\")\n",
    "\n",
    "nuek_processed.collect()\n",
    "\n",
    "# input(\"Press Enter to continue...5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Collect` —î –æ–¥–Ω—ñ—î—é –∑ –¥—ñ–π (Action) —è–∫—ñ –∑–∞–ø—É—Å–∫–∞—é—Ç—å Job.  \n",
    "–¢–∞–∫–∏–º–∏ –¥—ñ—è–º–∏ –≤ Spark —Ç–∞–∫–æ–∂ —î: `collect()`, `count()`, `save()`, `show()`\n",
    "\n",
    "–°–∫—Ä—ñ–Ω—à–æ—Ç–∏ Jobs:\n",
    "\n",
    "1. [Job 0](md.media/p2_0.png)  \n",
    "2. [Job 1](md.media/p2_1.png)  \n",
    "3. [Job 2](md.media/p2_2.png)  \n",
    "4. [Job 3](md.media/p2_3.png)  \n",
    "5. [Job 4](md.media/p2_4.png)  \n",
    "6. [Job 5](md.media/p2_5.png)  \n",
    "7. [Job 6](md.media/p2_6.png)  \n",
    "8. [Job 7](md.media/p2_7.png)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ó–∞–∫—Ä–∏–≤–∞—î–º–æ —Å–µ—Å—ñ—é Spark\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ß–∞—Å—Ç–∏–Ω–∞ 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í–∏–∫–æ—Ä–∏—Å—Ç–∞—î–º–æ —Ñ—É–Ω–∫—Ü—ñ—é `cache` –≤ –ø—Ä–æ–º—ñ–∂–Ω–æ–º—É —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ.\n",
    "\n",
    "     ‚òùüèª–§—É–Ω–∫—Ü—ñ—è cache() –≤ PySpark –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –¥–ª—è –∫–µ—à—É–≤–∞–Ω–Ω—è (–∑–±–µ—Ä—ñ–≥–∞–Ω–Ω—è –≤ –ø–∞–º'—è—Ç—ñ) –¥–∞–Ω–∏—Ö RDD (Resilient Distributed Dataset) –∞–±–æ DataFrame. –¶–µ –¥–æ–∑–≤–æ–ª—è—î –ø—Ä–∏—Å–∫–æ—Ä–∏—Ç–∏ –≤–∏–∫–æ–Ω–∞–Ω–Ω—è –ø–æ–¥–∞–ª—å—à–∏—Ö –¥—ñ–π (actions) –∞–±–æ –ø–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω—å (transformations), —è–∫—ñ –ø—Ä–∞—Ü—é—é—Ç—å –∑ —Ç–∏–º–∏ –∂ –¥–∞–Ω–∏–º–∏. –ö–µ—à—É–≤–∞–Ω–Ω—è –æ—Å–æ–±–ª–∏–≤–æ –∫–æ—Ä–∏—Å–Ω–µ, –∫–æ–ª–∏ –≤–∏ –≤–∏–∫–æ–Ω—É—î—Ç–µ –¥–µ–∫—ñ–ª—å–∫–∞ –æ–ø–µ—Ä–∞—Ü—ñ–π –Ω–∞ –æ–¥–Ω–æ–º—É –π —Ç–æ–º—É –∂ RDD –∞–±–æ DataFrame, –æ—Å–∫—ñ–ª—å–∫–∏ PySpark –Ω–µ –±—É–¥–µ –ø–æ–≤—Ç–æ—Ä–Ω–æ –æ–±—á–∏—Å–ª—é–≤–∞—Ç–∏ —Ç—ñ —Å–∞–º—ñ –¥–∞–Ω—ñ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**–Ø–∫ –ø—Ä–∞—Ü—é—î **`cache()`** :**\n",
    "\n",
    "**1. –ö–µ—à—É–≤–∞–Ω–Ω—è –≤ –ø–∞–º'—è—Ç—ñ.** –ö–æ–ª–∏ –≤–∏ –≤–∏–∫–ª–∏–∫–∞—î—Ç–µ `cache()` –Ω–∞ RDD –∞–±–æ DataFrame, –¥–∞–Ω—ñ –∑–±–µ—Ä—ñ–≥–∞—é—Ç—å—Å—è –≤ –ø–∞–º'—è—Ç—ñ (RAM) —É —Ä–æ–∑–ø–æ–¥—ñ–ª–µ–Ω–æ–º—É –≤–∏–≥–ª—è–¥—ñ –Ω–∞ –≤—Å—ñ—Ö –≤—É–∑–ª–∞—Ö –∫–ª–∞—Å—Ç–µ—Ä–∞. –¶–µ –¥–æ–∑–≤–æ–ª—è—î –ø—Ä–∏—Å–∫–æ—Ä–∏—Ç–∏ –ø–æ–¥–∞–ª—å—à—ñ –æ–±—á–∏—Å–ª–µ–Ω–Ω—è, –æ—Å–∫—ñ–ª—å–∫–∏ Spark –Ω–µ –±—É–¥–µ –∑–Ω–æ–≤—É –∑–∞–≤–∞–Ω—Ç–∞–∂—É–≤–∞—Ç–∏ –∞–±–æ –æ–±—á–∏—Å–ª—é–≤–∞—Ç–∏ —Ü—ñ –¥–∞–Ω—ñ.\n",
    "\n",
    "**2. –õ—ñ–Ω–∏–≤–µ –≤–∏–∫–æ–Ω–∞–Ω–Ω—è.** –í–∏–∫–ª–∏–∫ `cache()` –Ω–µ –ø—Ä–∏–∑–≤–æ–¥–∏—Ç—å –¥–æ –Ω–µ–≥–∞–π–Ω–æ–≥–æ –≤–∏–∫–æ–Ω–∞–Ω–Ω—è –æ–±—á–∏—Å–ª–µ–Ω—å. –õ–∏—à–µ –∫–æ–ª–∏ –≤–∏ –≤–∏–∫–æ–Ω—É—î—Ç–µ –¥—ñ—é (action), –Ω–∞–ø—Ä–∏–∫–ª–∞–¥, `count()`, `collect()`, –∞–±–æ `show()`, –¥–∞–Ω—ñ –±—É–¥—É—Ç—å –æ–±—á–∏—Å–ª–µ–Ω—ñ —Ç–∞ –∫–µ—à–æ–≤–∞–Ω—ñ.\n",
    "\n",
    "**3. –ú–µ—Ö–∞–Ω—ñ–∑–º –∑–±–µ—Ä—ñ–≥–∞–Ω–Ω—è.** –ó–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º, `cache()` –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î –ø–∞–º'—è—Ç—å (Memory). –û–¥–Ω–∞–∫, —è–∫—â–æ –¥–∞–Ω—ñ –Ω–µ –ø–æ–º—ñ—â–∞—é—Ç—å—Å—è –≤ –ø–∞–º'—è—Ç—å, Spark –±—É–¥–µ –∑–±–µ—Ä—ñ–≥–∞—Ç–∏ —ó—Ö –Ω–∞ –¥–∏—Å–∫—É.\n",
    "\n",
    "**4. –ö–æ–Ω—Ç—Ä–æ–ª—å –Ω–∞–¥ –∫–µ—à—É–≤–∞–Ω–Ω—è–º.** –ö–æ–ª–∏ –≤–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç–µ `cache()`, Spark –∑–±–µ—Ä—ñ–≥–∞—î –¥–∞–Ω—ñ –∑ —Ä—ñ–≤–Ω–µ–º –∑–±–µ—Ä—ñ–≥–∞–Ω–Ω—è `MEMORY_ONLY`. –Ø–∫—â–æ –≤–∏ —Ö–æ—á–µ—Ç–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ —ñ–Ω—à—ñ —Ä—ñ–≤–Ω—ñ –∑–±–µ—Ä—ñ–≥–∞–Ω–Ω—è, —Ç–∞–∫—ñ —è–∫ `MEMORY_AND_DISK`, –º–æ–∂–Ω–∞ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ –º–µ—Ç–æ–¥ `persist()`.\n",
    "\n",
    "    ‚òùüèª–í–∞–∂–ª–∏–≤–æ –∑–Ω–∞—Ç–∏, —â–æ –º–æ–∂–ª–∏–≤–æ –∑–±–µ—Ä—ñ–≥–∞—Ç–∏ —è–∫ –≤ –ø–∞–º'—è—Ç—ñ, —Ç–∞–∫ —ñ –Ω–∞ –¥–∏—Å–∫—É. –ó–±–µ—Ä—ñ–≥–∞–Ω–Ω—è –≤ –ø–∞–º'—è—Ç—ñ –Ω–∞–±–∞–≥–∞—Ç–æ –±—ñ–ª—å—à —Ä–æ–∑–ø–æ–≤—Å—é–¥–∂–µ–Ω–µ, –Ω–∞ –¥–∏—Å–∫—É ‚Äî –µ–∫–∑–æ—Ç–∏–∫–∞ üòâ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(unit_id='83', count=4)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# –°—Ç–≤–æ—Ä—é—î–º–æ —Å–µ—Å—ñ—é Spark\n",
    "spark = (\n",
    "    SparkSession.builder.master(\"local[*]\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"2\")\n",
    "    .appName(\"MyGoitSparkSandbox\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –¥–∞—Ç–∞—Å–µ—Ç\n",
    "nuek_df = (\n",
    "    spark.read.option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .csv(\"./csv/nuek-vuh3.csv\")\n",
    ")\n",
    "\n",
    "nuek_repart = nuek_df.repartition(2)\n",
    "\n",
    "nuek_processed_cached = (\n",
    "    nuek_repart.where(\"final_priority < 3\")\n",
    "    .select(\"unit_id\", \"final_priority\")\n",
    "    .groupBy(\"unit_id\")\n",
    "    .count()\n",
    "    .cache()\n",
    ")  # –î–æ–¥–∞–Ω–æ —Ñ—É–Ω–∫—Ü—ñ—é cache\n",
    "\n",
    "# –ü—Ä–æ–º—ñ–∂–Ω–∏–π action: collect\n",
    "nuek_processed_cached.collect()\n",
    "\n",
    "# –û—Å—å –¢–£–¢ –¥–æ–¥–∞–Ω–æ —Ä—è–¥–æ–∫\n",
    "nuek_processed = nuek_processed_cached.where(\"count>2\")\n",
    "\n",
    "nuek_processed.collect()\n",
    "\n",
    "# input(\"Press Enter to continue...5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ó–≤—ñ–ª—å–Ω—è—î–º–æ –ø–∞–º'—è—Ç—å –≤—ñ–¥ Dataframe\n",
    "nuek_processed_cached.unpersist()\n",
    "\n",
    "\n",
    "# –ó–∞–∫—Ä–∏–≤–∞—î–º–æ —Å–µ—Å—ñ—é Spark\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ó–∞–≤–¥—è–∫–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—é `cache` –º–∏ –∑–º–µ–Ω—à–∏–ª–∏ –∫—ñ–ª—å–∫—ñ—Å—Ç—å Jobs.\n",
    "\n",
    "–°–∫—Ä—ñ–Ω—à–æ—Ç–∏ Jobs:\n",
    "\n",
    "1. [All jobs](md.media/p3_jobs.png)\n",
    "1. [Job 0](md.media/p3_job_0.png)\n",
    "1. [Job 1](md.media/p3_job_1.png)\n",
    "1. [Job 2](md.media/p3_job_2.png)\n",
    "1. [Job 3](md.media/p3_job_3.png)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "goit_pyspark_sandbox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
